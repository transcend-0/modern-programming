# 现代程序设计
北航信管专业2022《现代程序设计》作业设计集

### 作业题目
#### week2 B站弹幕文本分析
利用python数据结构（list, dict, set等）完成简单的文本分析任务。弹幕是现下视频网站，尤其是短视频网站提供的关键功能之一。以B站为例，其有着特殊的弹幕文化，且在视频的不同部分往往会有不同话题的弹幕：比如在视频开头会出 现“来啦”“x小时前”“第一!”;在up主暗示一键三连之后常常会出现“下次一定”或者“你币有 了”;和up主建立默契之后，观众可以判断视频是否有恰饭，往往在广告之前会出现“要素察 觉”“恰饭”“快跑”等等。因此，弹幕经常被作为测度用户（viewer）与视频作者（up主）之间交互行为的关键数据。
本次作业提供的数据来自B站某知名up主，已上传至课程资料的data目录下，数据格式说明如下。 a. 弹幕文件：danmuku.csv，为2799000 rows × 3 columns，本次作业仅使用第一列，即弹幕的文本内容。 b . 停用词表示例，stopwords_list.txt 
请大家尝试完成以下数据分析任务： 
1. 使用danmuku.csv，其中一个弹幕可以视为一个文档（document），读入文档并分词（可以使用jieba或pyltp）。 
2. 过滤停用词（可用stopwords_list.txt，或自己进一步扩充）并统计词频，输出特定数目的高频词和低频词进行观察。建议将停用词提前加入到jieba等分词工具的自定义词典中，避免停用词未被正确分词。 
3. 根据词频进行特征词筛选，如只保留高频词，删除低频词（出现次数少于5之类），并得到特征词组成的特征集。 
4. 利用特征集为每一条弹幕生成向量表示，可以是0，1表示（one-hot，即该特征词在弹幕中是否出现）也可以是出现次数的表示（该特征词在弹幕中出现了多少次）。注意，可能出现一些过短的弹幕，建议直接过滤掉。 
5. 利用该向量表示，随机找几条弹幕，计算不同弹幕间的语义相似度，可尝试多种方式，如欧几里得距离或者余弦相似度等，并观察距离小的样本对和距离大的样本对是否在语义上确实存在明显的差别。请思考，这种方法有无可能帮助我们找到最有代表性的弹幕？ 
6. （附加）能不能对高频词（如top 50之类）进行可视化呈现（WordCloud包）？ 
7. （附加）能不能考虑别的特征词构建思路，如常用的TF-IDF，即一方面词的频率要高，另一方面，词出现的文档数越少越好，观察其与仅利用词频所得的结果有何差异，哪个更好？ 
8. （附加）了解一下word2vec等深度学习中常用的词向量表征（如gensim和pyltp中均有相关的库），并思考如果用这种形式的话，那么一条弹幕会被表示成什么形式？弹幕之间计算相似性的时候，会带来哪些新的问题？ 注意：不要使用jieba等库中提供的函数实现特征词抽取和文档表示，要求自己使用相关数据结构来实现；要通过函数对代码进行封装，并在main函数中调用。

#### week3 情感分析
情绪理解是文本处理里最常见任务之一。现提供一个五类情绪字典（由情绪词组成，5个文件，人工标注），实现一个情绪分析工具，并利用该工具对10000条新浪微博进行测试和分析（一行一条微博）。微博数据见课程资料中提供的weibo.txt（200万条，包括地理位置，文本和发布时间），字典数据见公开数据中的emotion lexicon（https://doi.org/10.6084/m9.figshare.12163569.v2）。请按要求用函数进行功能封装，并在main中调用测试，鼓励尝试不同方式的可视化。 
1. 实现一个函数，对微博数据进行清洗，去除噪声（如url等），过滤停用词。注意分词的时候应该将情绪词典加入Jieba或pyltp的自定义词典，以提高这些情绪词的识别能力。 
2. 实现两个函数，实现一条微博的情绪分析，返其情绪向量或情绪值。目前有两种方法，一是认为一条微博的情绪是混合的，即一共有n个情绪词，如果joy有n1个，则joy的比例是n1/n；二是认为一条微博的情绪是唯一的，即n个情绪词里，anger的情绪词最多，则该微博的情绪应该为angry。注意，这里要求用闭包实现，尤其是要利用闭包实现一次加载情绪词典且局部变量持久化的特点。同时，也要注意考虑一些特别的情况，如无情绪词出现，不同情绪的情绪词出现数目一样等，并予以处理（如定义为无情绪，便于在后面的分析中去除）。 
3. 微博中包含时间，可以讨论不同时间情绪比例的变化趋势，实现一个函数，可以通过参数来控制并返回对应情绪的时间模式，如joy的小时模式，sadness的周模式等。 
4. 微博中包含空间，可以讨论情绪的空间分布，实现一个函数，可以通过参数来控制并返回对应情绪的空间分布，即围绕某个中心点，随着半径增加该情绪所占比例的变化，中心点可默认值可以是城市的中心位置。 
5. （附加）讨论字典方法进行情绪理解的优缺点，有无可能进一步扩充字典来提高情绪识别的准确率？如何扩充，有无自动或半自动的扩充思路？ 
6. （附加）可否对情绪的时间和空间分布进行可视化？（如通过matplotlib绘制曲线，或者用pyecharts（注意版本的兼容性）在地图上标注不同的情绪） 
7. （附加）思考情绪时空模式的管理意义，如营销等。 注意：如果规模太大处理不了，可以酌情处理一部分。

#### week4 社交网络python包构建
图是非常重要的一种数据结构，常用于描述社交网络。本次作业提供了twitch_gamers数据集（见在线平台课程资料twitch_gamers.zip），希望基于该数据，构建一个python 程序包，并在对应模块中实现：读取并存储节点信息，建立无向的社交网络，以及实现相关统计和可视化功能。
1. 该数据中， large_twitch_features.csv 中每一行为一个节点的相关属性，large_twitch_edges.csv中第一行为一条边。具体详细信息请阅读README.txt。
2. 建立包GraphStat，实现网络的构建、分析与可视化。其中：
    1. 包Graph，用以实现点和图结构的创建，以及相关的基础统计功能
        1. 实现node.py模块
            1. 实现函数init_node()，从数据文件中加载所有节点及其属性；
            2. 实现函数print_node()，利用format函数或f-string，输出某节点的属性
        2. 实现graph.py模块，实现图结构的序列化存储和加载。
        3. 实现stat.py模块，进行基础的统计分析
            1. 计算网络的节点数、边数、平均度等并返回
            2. 统计某个节点属性的分布
    2. 包Visualization， 基于上述构建的图和节点结构，利用pyecharts或matplotlib绘制相关的统计结果
        1. 实现plotgraph.py模块，绘制网络的局部结构（如某个节点及其所有邻居所组成的ego网络）
        2. 实现plotnodes.py模块，绘制节点的属性分布，并提供结果的输出或文件保存（图片）
3. （附加）观察所构建的网络在平均度，度分布，甚至局部结构上的结果和形态，讨论如何用这些数据来对节点进行排序或者挑选，比如假设你想在这个网络上营销一个新游戏，应该
找那些节点来“试用”，以快速地产生口碑？了解一些节点重要性的一些常见指标。
4.（附加）了解并使用Gephi工具，尝试网络结构的可视化与社团分析等。

#### week5 实现文本处理的Tokenizer类
深度学习处理自然语言时，会常常用到Tokenizer（https://huggingface.co/transformers/tokenizer_summary.html）。
简单来说，就是按照预先定义好的词典，把文本编码成整数序列的过程。深度学习模型进行文本挖掘任务时会经常需要处理这种编码过的序列。在构建过程中，有时候我们希望句子的长度能够整齐，所以会规定一个特殊的号码[PAD]=0，代表填充位。具体地，例如词典为 {'[PAD]': 0, ‘我’:1, '是': 2, '北京': 3, '大学生': 4, '的': 5 }，那么"我是北京的大学生" 将被编码为 [1, 2, 3, 5, 4 ] ；如果需要句子长度为8，那么我们在后面填充[PAD]，得到 [1,2,3,5,4,0,0,0]。

现在请编程实现一个基础的中文Tokenizer类，分别实现按字（深度学习中也常用该方式）或按词（通过jieba分词）进行编码，在本次作业提供的文本数据集上进行测试，同时讨论tokenizer方法与第二周作业中的one-hot方法编码之间的区别和优劣。
1. 代码结构：
    1. \_\_init\_\_(self, chars,coding='c',PAD=0) 输入将要需要操作的文本（一个字符串的列表），这里需要完成词典的构建（即汉字到正整数的唯一映射的确定）。注意构建词典是要根据coding来选择按词构建（coding='w')，还是按字构建，默认按字构建；PAD默认为0。
    2. tokenize(self, sentence) 输入一句话，返回分词(字）后的字符列表(list_of_chars)。
    3. encode(self, list_of_chars) 输入字符(字或者词）的字符列表，返回转换后的数字列表(tokens)。
    4. trim(self, tokens, seq_len) 输入数字列表tokens，整理数字列表的长度。不足seq_len的部分用PAD补足，超过的部分截断。
    5. decode(self, tokens) 将模型输出的数字列表翻译回句子。如果有PAD，输出'[PAD]'。
    6. encode_all(self，seq_len) 返回所有文本(chars)的长度为seq_len的tokens。
2. 提示：seq_len是句子的长度，实际任务中如何确定一个合适的长度，请以本次作业中的文本数据为例，通过文本的长度分布来进行观察和讨论。
3.（附加）尝试使用BERT预训练模型（https://huggingface.co/bert-base-chinese）
中提供的tokenizer对本次样本中的文本进行编码，并使用其预训练模型得到句子的表征向量。抽取本次数据中的一条文本，并查找其相似文本，并与第二周中使用word2wec得到的结果相比较，分析两者优劣。

#### week6 面向对象编程+图片处理
1. 实现基类Filter，至少包括两个数据属性，一个属性是待处理的图片实例，即PIL库的Image 实例，另一个是参数列表，用以存储可能使用参数的滤波器的参数；至少包括一个方法属性，即filter()方法，能够对Image 实例的特定处理。但在该类中并不需要进行实现，其实现细节应该交给子类。
2. 实现Filter 类的多个子类，分别实现对图片的一些滤波处理，至少应进行边缘提取，锐化，模糊及大小调整四类操作，也即应实现至少4 个子类，分别对基类中的filter()方法进行实现。注意，并不需要真正实现对应的操作，可简单地通过PIL 中的Image和ImageFilter 模块来实现。具体可参见
https://pillow.readthedocs.io/en/stable/reference/ImageFilter.html。
3. 实现类ImageShop，其至少包含四个数据属性，分别是图片格式，图片文件（应该支持目录），存储图片实例(Image 实例)的列表以及存储处理过的图片（如果需要的话）。至少包含如下方法属性，分别是从路径加载特定格式的图片（load_images()，应加载文件或目录中的所有特定格式图片）；处理图片的内部方法__batch_ps(Filter),利有某个过滤器对所有图片进行处理；批量处理图片的对外公开方法（batch_ps()），注意该方法要至少有一个操作参数，且该参数可以不定长，即可以同时进行若干操作（如调整大小并模糊等），其参数可定义成一种特定格式的tuple 输入，比如（操作名，参数），根据操作名生成对应的Filter 子类并调用 __batch_ps 来完成批处理；处理效果显示（display（）），注意该方法应该有参数，如考虑多图片呈现可能需要行，列以及每张图片的大小，以及最多显示多少张等，可通过matplotlib 中的subplot 等来实现；处理结果输出（save()），该方法应该有输出路径或输出格式等参数。
4. 实现测试类TestImageShop，对该类进行测试，指定图片路径，指定要进行的操作（如有参数也可应提供），并对执行结果进行呈现和存储。
5. 附加：观察一些经过过滤后图片的变化，思考这些处理对图片本身的一些相关的计算，如图片的相似性等有无影响，并进行简单的实验验证。另外，这些预处理本身对下游的机器学习模型会带来哪些好处？
6. 附加：进一步了解torchvision.transforms 中对图片的一些基础操作，以及这些基础操作在数据增强中的常见应用，如作业中的资料：实践教程｜13 个Pytorch 图像增强方法总
结.html。
7. 附加：进一步了解一下深度卷积网络以及其在计算机视觉中的应用现状。

#### week7 面向对象编程+数据分析
经济管理中通常有大量的数据以csv等结构化格式存在，如本次作业要用的空气质量数据。数据见在线平台，格式说明如
https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data。 
1. 至少实现一个数据分析类，以提供数据的读取及基本的时间（如某区域某类型污染物随时间的变化）和空间分析（某时间点或时间段北京空气质量的空间分布态势）方法。 
2. 至少实现一个数据可视化类，以提供上述时空分析结果的可视化，如以曲线、饼、地图等形式对结果进行呈现。 
3. 如果数据中包含空值等异常值（可人工注入错误数据以测试异常抛出与处理的逻辑），在进行数据分析以及可视化前需要检查数据。因此需要实现NotNumError类，继承ValueError，并加入新属性region, year，month，day,hour，pollutant，对数据进行检测，若取到的一列数据中包含空值等明显错误，则抛出该异常，并提供异常信息。在此基础上，利用try except捕获该异常，打印异常信息，并对应位置的数据进行适当的填充。 
4. （附加）污染物含量与气象状态本身是否有相关性？请丰富数据分析类和数据可视化类，增加关于这些相关性探索的方法。 
5. （附加）思考不同区域时间变化的趋势及差异的管理意义。 提示：了解并使用csv（标准库），openpyxl等来读取csv等结构化文件，或直接视为一般文本文件读取。

#### week8 函数式编程
请用函数或类实现不同功能的装饰器并进行充分的测试。 
1. 请用函数实现装饰器。部分函数往往需要将模型或者数据处理结果保存下来，但实际调用时却因为路径设置错误等原因导致文件无法存储，浪费大量的时间重复运行程序。请实现一个装饰器，接收函数的路径参数，检查路径对应文件夹是否存在，若不存在，则给出提示，并在提示后由系统自动创建对应的文件夹。 
2. 请用类实现一个装饰器。部分函数可能需要花费较长时间才能完成，请实现一个装饰类，其能够在被装饰函数结束后通过声音给用户发出通知。了解并使用一下playsound或其他声音文件处理的库。另外，可否对根据返回值的类型，比如整数，元组等，来实现不同的声音通知？如果返回值有多个，可否多次按类型依次播放？ 
3. 请用类或者函数实现一个装饰器。部分函数可能会在运行过程中输出大量的中间状态或者中间结果，这些信息往往在程序出问题时利于调试，但由于输出内容过多，可能在控制台中无法全部查看。请实现一个装饰器，其能够将被装饰函数在运行过程中的所有的输出（通过print）全部保存在特定的一个文件中。 
4. 实现一个类，在其中提供一些方法模拟耗时耗内存的一些操作，如大的数据结构生成、遍历、写入文件序列化等，并通过其体验line_profiler、memory_profiler、tqdm、pysnooper等装饰器的相关功能。

#### week9 生成器和迭代器
生成器和迭代器有两种常见的使用场景。 

一. 后项需要前项导出，且无法通过列表推导式生成。例如，时间序列中的“随机游走”便是一种满足上述条件的序列数据。其公式为$$X_t = \mu + X_｛t-1｝ + w_t$$，其中$\mu$为漂移量，$w_｛t｝$是满足某种条件的独立同分布的随机变量，这里假设其服从正态分布N（0, $\sigma^2$）。本题要求写出实现该功能的迭代器函数。具体要求如下： 
1. 实现random_walk生成器，输入参数$\mu$, $X_0$, $\sigma^2$，$N$，函数将迭代返回N个随机游走生成的变量。 
2. 利用zip，实现拼合多个random_walk的生成器，以生成一组时间上对齐的多维随机游走序列。 

二. 需要迭代的内容数据量过大，无法一次性加载。例如，在图像相关的深度学习任务中，由于数据总量过大，一次性加载全部数据耗时过长、内存占用过大，因此一般会采用批量加载数据的方法。（注：实际应用中由于需要进行采样等操作，通常数据加载类的实现原理更接近字典，例如pytorch中的Dataset类。）现提供文件FaceImages.zip（http://vis-www.cs.umass.edu/fddb/originalPics.tar.gz，其中包含5000余张人脸图片。要求设计FaceDataset类，实现图片数据的加载。具体要求： 
1. 类接收图片路径列表 
2. 类支持将一张图片数据以ndarray的形式返回（可以利用PIL库实现）。 3. 实现__iter__方法。 4. 实现__next__方法，根据类内的图片路径列表，迭代地加载并以ndarray形式返回图片数据。 请实现上述生成器和迭代器并进行测试。

#### week10 面向对象编程
在使用python时，我们经常会用到许多工具库，它们提供了较为方便的函数调用。但是仍然会有一些情况，例如数据类型或格式不符合函数要求，参数存在差异等，使得调用前需要对数据进行额外处理。本次作业要求基于matplotlib，wordcloud，PIL, imageio等绘图库的绘制函数，设计并实现适配器抽象类和不同的适配类，以实现不同类型数据的多样化可视。具体要求如下：
1. 要求设计抽象类Plotter，至少包含抽象方法plot(data, *args, **kwargs)方法，以期通过不同子类的具体实现来支持多类型数据的绘制，至少包括数值型数据，文本，图片等。
2. 实现类PointPlotter, 实现数据点型数据的绘制，即输入数据为[(x,y)...]型，每个元素为一个Point类的实例。
3. 实现类ArrayPlotter, 实现多维数组型数据的绘制，即输入数据可能是[[x1,x2...],[y1,y2...]]或者[[x1,x2...],[y1,y2...],[z1,z2...]]。
4. 实现类TextPlotter，实现文本型数据的绘制，即输入数据为一段或多段文本，应进行切词，关键词选择（根据频率或tf-idf)，继而生成词云。
5. 实现类ImagePlotter，实现图片型数据的绘制，即输入数据为图片的路径或者图片内容（可以是多张图片），呈现图片并按某种布局组织（如2x2等)。
6. 实现类GifPlotter, 支持一组图片序列的可视化（通过文件路径或图片内容输入），但输出是gif格式的动态图。
7. （附加）在3中，如果多维数组超过3维，可否支持pca等降维并绘制，实现类KeyFeaturePlooter？（了解pca或者TSNE）
8. （附加）如果输入是一段音频（比如mp3文件），如何进行绘制，实现类MusickPlotter？（了解librosa里的display模块）
9. （附加）在6中，如果输入是一段落视频的话，能否通过帧采样，将视频绘制为gif并输出为微信表情包，实现类VideoPlotter？（了解cv2)

#### week11  进程
MapReduce是利用多进程并行处理文件数据的典型场景。作为一种编程模型，其甚至被称为Google的”三驾马车“之一(尽管目前由于内存计算等的普及已经被逐渐淘汰)。在编程模型中，Map进行任务处理，Reduce进行结果归约。本周作业要求利用Python多进程实现MapReduce模型下的文档库（搜狐新闻数据(SogouCS)（下载地址：https://www.sogou.com/labs/resource/cs.php）
，注意仅使用页面内容，即新闻正文）词频统计功能。具体地：
1. Map进程读取文档并进行词频统计，返回该文本的词频统计结果。
2. Reduce进程收集所有Map进程提供的文档词频统计，更新总的文档库词频，并在所有map完成后保存总的词频到文件。
3. 主进程可提前读入所有的文档的路径列表，供多个Map进程竞争获取文档路径；或由主进程根据Map进程的数目进行分发；或者单独实现一个分发进程，与多个Map进程通信。
4. 记录程序运行时间，比较不同Map进程数量对运行时间的影响，可以做出运行时间-进程数目的曲线并进行简要分析。进程数量并非越多越好。

#### week12 线程+爬虫
以爬取网易云歌单为例，练习多线程的使用。 
1. 获取一个分类下的所有歌单的id。观察url可以发现其页码规律：https://music.163.com/#/discover/playlist/?order=hot&cat=%E8%AF%B4%E5%94%B1&limit=35&offset=35。
offset是本页开始的数据位置，第1页是0，第2页是35。分类参数是utf-8编码后的汉字"说唱"，使用str.encode可以处理。 
2. 对每个id，获取歌单的详细信息，至少包括：歌单的封面图片（需把图片保存到本地）、歌单标题、创建者id、创建者昵称、介绍、歌曲数量、播放量、添加到播放列表次数、分享次数、评论数。可以自行实现其他信息的获取。基本信息汇总到同一张表中，以csv文件保存。https://music.163.com/playlist?id=3037221581 
3. 要求使用生产者-消费者模式实现，要求1作为生产者，每次请求后产生新的任务交给消费者，消费者执行要求2。
4.（附加）爬虫程序往往需要稳定运行较长的时间，因此如果你的程序突然中断或异常（比如网络或被封），如何能够快速从断点重启？
5.（附加）爬虫程序往往需要比较友好的状态输出，因此可否专门有一个线程 动态地进行输出更新，来显示当前的状态，比如程序连续运行的时长，要完成的 总页面数，其中有多少已被爬取，已收集的文件占用了多少空间，大概还需要多 少时间才能完成，预计需要耗费多少硬盘空间等。

#### week13 socket编程
一：利用socket和多线程，实现支持多人对话的聊天室。具体地，实现Manager和Chatter 两个类，Chatter只需和Manager之间建立一对一联系，而Manager则负责广播或转发所有用户的消息。请在实际中找个场景运用。
相关要求如下：
1. 实现Manager类, 服务器，管理成员进入和离开聊天室，接收成员消息并广播
2. 实现Chatter类, 用户, 向管理员发送加入和退出请求，发送和接收消息
3. Manager类使用多线程服务多个用户
4. Chatter用户发送和接收消息需要依赖不同线程进行
5. Manager类具备定向转发功能，比如Chatter可以在消息中通过@指定特定用户，这样Manager将仅转发给被指定用户。
6. Chatter在离开时，自动保存聊天记录到硬盘（包括时间、发信人，信息）。
7. Manager也应保存所有聊天室记录到硬盘。

二：根据udp的演示例子，实现一个简单的“无声监控”服务程序，并在实际上找个场景运用。
1. 服务端要支持多线程，客户端退出时要停止相应子线程。
2. 测试时要实现多个接入测试，即有多个客户端获取视频流。
3. 客户端按一定时长存储视频流（比如每10分钟存储一个文件）。
4. 服务端记录客户端接入的日志（哪个客户端，何时接入，何时离开）文件。
5. 要用类进行封装。
6. （附加）感兴趣的同学了解一下组播，并尝试用组播来减轻服务端压力。

#### week14 协程
协程有时被称为“微线程”，因为二者有类似的使用场景。和线程相比，协程占用资源更少、切换迅速，且实现简单（如协程是协作式调用，一般不用考虑资源的抢占，所以大部分情况下不需要通过同步原语来避免冲突）。通常协程被用在高并发的IO场景中，因此本周要求在第十二次作业的基础上，利用协程重写并扩充已有爬虫的功能。
注意：可以使用aiohttp，aiofiles等第三方库。

#### week15 数据库编程
本周要求在第14周的网络爬虫基础上设计用于存储爬取内容的数据库。由于爬取内容包括文本这样的非结构化数据，且数据间没有复杂的关系结构，为了操作简便，采用结构（PostgreSQL)或非结构化数据库（mongodb）对数据进行处理。具体要求如下：
1. 安装所选择数据库，并启动数据库，设用户以及密码，供后续存储文档使用
2. 实现python对数据库的连接
3. 创建数据表（集合）voa，在已有的爬虫系统中补充相关功能，将爬取到的数据存入数据库，要求包括如下字段：ID，title，text，author，date，mp3_path（对mp3文件，在表中只存储路径即可），related_articles。其中ID可在url中获取，title，author，date，related_articles等内容可在数据页面内找到。
4. （附加）编写相关python函数，实现基于数据库的简单查询功能(可以加入其他搜索字段，例如时间，作者等，对应通用的网站搜索功能) 。